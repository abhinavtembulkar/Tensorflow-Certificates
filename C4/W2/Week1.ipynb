{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf Series Dataset preparation\n",
    "In this notebook, we will use tensorflow library to create <br>\n",
    "Training and validation dataset, instead of doing this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_RangeDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "print(dataset)\n",
    "\n",
    "for data in dataset:\n",
    "    print(data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
      "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "STEP = 1\n",
    "\n",
    "windowed_dataset = tf.data.Dataset.window(dataset, WINDOW_SIZE, STEP)\n",
    "for data in windowed_dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print the data, iterate over it. Since each element is a array here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n",
      "[6, 7, 8, 9]\n",
      "[7, 8, 9]\n",
      "[8, 9]\n",
      "[9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 14:08:50.972620: W tensorflow/core/framework/dataset.cc:959] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "for data in windowed_dataset:\n",
    "    print([x.numpy() for x in data ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To throw away the unequal sized windows, ie. windows having less than 5 elements, use these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[2, 3, 4, 5, 6]\n",
      "[3, 4, 5, 6, 7]\n",
      "[4, 5, 6, 7, 8]\n",
      "[5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "window_dataset = tf.data.Dataset.window(dataset, size=WINDOW_SIZE, shift=STEP, drop_remainder=True)\n",
    "\n",
    "for data in window_dataset:\n",
    "    print([x.numpy() for x in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert your window dataset to a Tensor batch to feed into our model , use this function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[1 2 3 4 5]\n",
      "[2 3 4 5 6]\n",
      "[3 4 5 6 7]\n",
      "[4 5 6 7 8]\n",
      "[5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "flat_dataset = window_dataset.flat_map(lambda x: x.batch(5))\n",
    "for window in flat_dataset:\n",
    "    # Each window here is a tensor\n",
    "    print(window.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset into features & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3] 4\n",
      "[1 2 3 4] 5\n",
      "[2 3 4 5] 6\n",
      "[3 4 5 6] 7\n",
      "[4 5 6 7] 8\n",
      "[5 6 7 8] 9\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "w_dataset = tf.data.Dataset.window(dataset, WINDOW_SIZE, STEP, drop_remainder=True)\n",
    "\n",
    "w_dataset_fm = w_dataset.flat_map(lambda w: w.batch(WINDOW_SIZE))\n",
    "w_map = w_dataset_fm.map(lambda w_tensor: (w_tensor[:-1], w_tensor[-1]))\n",
    "\n",
    "for sample in w_map:\n",
    "    x, y = sample\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle this batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4] 5\n",
      "[0 1 2 3] 4\n",
      "[2 3 4 5] 6\n",
      "[3 4 5 6] 7\n",
      "[5 6 7 8] 9\n",
      "[4 5 6 7] 8\n"
     ]
    }
   ],
   "source": [
    "w_shuffle_map = w_map.shuffle(10)\n",
    "for x, y in w_shuffle_map:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add another sample to each batch for prefetching. <br>\n",
    "The dataset is generated at run-time by tf. This is done to<br> \n",
    "reduce runtime-memory consumption by our model. Model only <br>\n",
    "consumes one sample, but prefetching one sample beforehand <br>\n",
    "reduces time taken to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 5 6 7]\n",
      " [0 1 2 3]] [8 4]\n",
      "[[5 6 7 8]\n",
      " [1 2 3 4]] [9 5]\n",
      "[[2 3 4 5]\n",
      " [3 4 5 6]] [6 7]\n"
     ]
    }
   ],
   "source": [
    "prepared_dataset = w_shuffle_map.batch(2).prefetch(1)\n",
    "\n",
    "for (x, y) in prepared_dataset:\n",
    "    print(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
